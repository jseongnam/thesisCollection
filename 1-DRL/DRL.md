# 제목 : Playing Atari with Deep Reinforcement Learning
## Abstract
강화학습을 이용하여 고차원 감각 입력으로부터 직접 제어정책을 성공적으로 학습한 최초의 딥러닝 모델을 제시한다. 모델은 Q-learning의 변형으로 훈련된 컨볼루션 신경망이다.

입력은 원시 픽셀이고, 출력은 미래를 추정하는 값 함수이다. 보상은 아케이드 학습 환경의 아타리 2600 게임 7개에 아키텍처나 학습 알고리즘을 조정하지 않고 적용한다.

결과 : 6개의 게임에서 이전의 모든 접근 방식을 능가하고 다음을 능가한다는 것을 발견.

## Introduction
이 논문은, 컨볼루션 신경망이 기존의 강화학습의 딥러닝의 문제를 극복하고 학습할 수 있음을 보여준다.

## Background
에이전트가 환경 E와 상호작용하는 작업을 고려한다. 아타리 에뮬레이터, 일련의 행동, 관찰 및 보상, 각 시간 단계에서 에이전트는 행동을 선택한다.
합법적인 게임 액션 집합에서 A = {1,...,K} 액션은 에뮬레이터로 전달되며, 내부 상태와 게임 점수를 수정한다. 일반적으로 E는 확률적일 수 있다.
에뮬레이터의 에이전트가 내부 상태를 관찰하지 않고, 그 대신 이미지 xt ∈ R 을 관찰한다. 게임 점수의 변화를 나타내는 rt가 있다. 일반적응로 게임 점수는 다음과 같다.
행동 및 관찰의 전체 사전 시퀀스 : 행동에 대한 피드백만 수신할 수 있다. 수천 번의 시간이 흐른 뒤에, 에이전트는 현재 화면의 이미지만 관찰하기 때문에, 작업이 부분적으로 관찰되고, 현재 상황을 완전히 이해하는 것은 불가능하다. 현재 화면 xt로부터만, 우리는 행동과 관찰의 순서를 고려한다. 
St =x1, a1, x2, ..., at-1, xt 이 시퀀스들에 의존하는 게임 전략을 배운다. 
에이전트의 목표는 감소율 γ로 할인된 미래 보상을 최대화하는 방식으로 에뮬레이터와 상호작용하는 것.
우리는 일반적으로 미래 보상이 시간 단계마다 γ의 지수적으로 할인된다는 가정을 하며, 시간 t에서 미래 할인된 반환을 다음과 같이 정의한다.

$R_t = \sum_{t_0 = t}^{T} \gamma^{t_0 - t} r_{t_0}$
