# 제목 : Playing Atari with Deep Reinforcement Learning
## Abstract
강화학습을 이용하여 고차원 감각 입력으로부터 직접 제어정책을 성공적으로 학습한 최초의 딥러닝 모델을 제시한다. 모델은 Q-learning의 변형으로 훈련된 컨볼루션 신경망이다.

입력은 원시 픽셀이고, 출력은 미래를 추정하는 값 함수이다. 보상은 아케이드 학습 환경의 아타리 2600 게임 7개에 아키텍처나 학습 알고리즘을 조정하지 않고 적용한다.

결과 : 6개의 게임에서 이전의 모든 접근 방식을 능가하고 다음을 능가한다는 것을 발견.

## Introduction
이 논문은, 컨볼루션 신경망이 기존의 강화학습의 딥러닝의 문제를 극복하고 학습할 수 있음을 보여준다.

## Background
에이전트가 환경 E와 상호작용하는 작업을 고려한다. 아타리 에뮬레이터, 일련의 행동, 관찰 및 보상, 각 시간 단계에서 에이전트는 행동을 선택한다.
합법적인 게임 액션 집합에서 A = {1,...,K} 액션은 에뮬레이터로 전달되며, 내부 상태와 게임 점수를 수정한다. 일반적으로 E는 확률적일 수 있다.
에뮬레이터의 에이전트가 내부 상태를 관찰하지 않고, 그 대신 이미지 xt ∈ R 을 관찰한다. 게임 점수의 변화를 나타내는 rt가 있다. 일반적응로 게임 점수는 다음과 같다.
행동 및 관찰의 전체 사전 시퀀스 : 행동에 대한 피드백만 수신할 수 있다. 수천 번의 시간이 흐른 뒤에, 에이전트는 현재 화면의 이미지만 관찰하기 때문에, 작업이 부분적으로 관찰되고, 현재 상황을 완전히 이해하는 것은 불가능하다. 현재 화면 xt로부터만, 우리는 행동과 관찰의 순서를 고려한다. 
St =x1, a1, x2, ..., at-1, xt 이 시퀀스들에 의존하는 게임 전략을 배운다. 
에이전트의 목표는 감소율 γ로 할인된 미래 보상을 최대화하는 방식으로 에뮬레이터와 상호작용하는 것.
우리는 일반적으로 미래 보상이 시간 단계마다 γ의 지수적으로 할인된다는 가정을 하며, 시간 t에서 미래 할인된 반환을 다음과 같이 정의한다.

$R_t = \sum_{t_0 = t}^{T} \gamma^{t_0 - t} r_{t_0}$

여기서 T는 게임이 종료되는 시간 단계이다.
우리는 최적의 행동-가치 함수 Q∗(s, a)을 다음과 같이 정의한다.
먼저, 이는 일부 시퀀스 s를 보고 일부 작업 a를 수행한 후에 어떤 전략을 따를 때 얻을 수 있는 최대 기대 반환이다.

$Q^*(s, a) = \max_{\pi} \mathbb{E} [R_t|s_t = s, a_t = a, \pi]$

여기서 $\pi$ 는 시퀀스를 작업(또는 작업에 대한 분포)로 매핑하는 정책이다.
최적의 행동-가치함수는 벨만 방정식이라는 중요한 동일성을 따른다. 이는 다음 직관에 기반한다.
$s_0$ 의 최적 값 $Q_*(s_0,a_0)$ 이 모든 가능한 작업 $a_0$ 에 대해 알려져 있다면, 
최적의 전략은 기대값  $r + γQ_∗(s_0, a_0)$ 을 최대화하는 작업 $a_0$을 선택하는 것.

$Q^*(s, a) = E_{s_0 \sim \epsilon} [r + \gamma \max_{a_0} Q^*(s_0, a_0) | s, a]$

많은 강화 학습 알고리즘의 기본 아이디어는 벨만 방정식을 반복적인 업데이트로 사용하여 행동-가치 함수를 추정하는 것이다.
이러한 값 반복 알고리즘은 i → ∞에 대해 $Q_i$ → $Q_∗$로 수렴한다.
실제로, 이러한 기본 접근 방식은 각 시퀀스에 대해 행동-가치 함수를 별도로 추정하므로 전혀 실용적이지 못하다.
대신, 행동-가치 함수를 추정하기 위해 함수 근사기를 사용하는 것이 일반적이다. 
이 때 행동-가치 함수는 Q(s, a; θ) ≈ Q∗(s, a) 로 표기된다.
강화학습 커뮤니티에서는 일반적으로 선형 함수 근사기를 사용하지만, 때로는 신경망과 같은 비선형 함수 근사기를 대신 사용한다.

우리는 가중치 θ 를 가진 신경망 함수 근사기를 Q-network 이라고 한다. Q-network은 각 반복에서 변하는 손실 함수 $L_i(θ_i)$ 를 최소화하여 훈련될 수 있다.

$L_i (θ_i) = E_{s,a∼ρ(·)}[(y_i − Q (s, a; θ_i))2]$ 
여기서 $y_i = E_{s0∼E} [r + γ max_{a0} Q(s_0, a_0; θ_{i−1})|s, a]$

## Related Work

강화 학습의 가장 잘 알려진 성공 사례 중 하나는 TD-Gammon입니다. 이는 완전히 강화 학습과 셀프 플레이로 학습한 배경모노 게임 프로그램으로, 초인적인 수준의 게임 능력을 달성했습니다. TD-Gammon은 Q-learning과 유사한 모델 없는 강화 학습 알고리즘을 사용하고, 한 개의 은닉층을 가진 다층 퍼셉트론을 사용하여 가치 함수를 근사했습니다.

그러나, TD-Gammon에 대한 조사를 이어가는 초기 시도는 체스, 바둑, 체커와 같은 게임에 동일한 방법을 적용한 것들은 덜 성공적이었습니다. 이로 인해 TD-Gammon 접근법이 배경모노에서만 작동하는 특수한 경우라는 보편적인 믿음이 생겼는데, 아마도 주사위 굴림의 확률성이 상태 공간을 탐색하고 가치 함수를 특히 부드럽게 만든다는 이유 때문일 것입니다.

또한, Q-learning과 같은 모델 없는 강화 학습 알고리즘을 비선형 함수 근사기와 결합하거나, 오프-정책 학습과 같은 것을 함께 사용할 경우 Q-네트워크가 발산할 수 있다는 것이 밝혀졌습니다. 이후, 강화 학습 분야의 주요 작업은 수렴 보장이 더 우수한 선형 함수 근사기에 중점을 두었습니다.

더 최근에는 심층 학습과 강화 학습을 결합하는 데 관심이 다시 생겼습니다. 심층 신경망은 환경 E를 추정하는 데 사용되었으며, 제한된 볼츠만 머신은 가치 함수나 정책을 추정하는 데 사용되었습니다. 또한, Q-learning의 발산 문제는 경사 시간 차 메서드를 통해 일부 해결되었습니다. 이러한 방법은 비선형 함수 근사기를 사용하여 고정된 정책을 평가할 때 수렴하거나, Q-learning의 제한된 변형을 사용하여 선형 함수 근사기를 사용하여 제어 정책을 학습할 때 수렴함이 입증되었습니다. 그러나 이러한 방법은 아직 비선형 제어로 확장되지 않았습니다.

우리의 접근 방식과 가장 유사한 이전 연구는 신경 맞춤 Q-learning (NFQ)입니다. NFQ는 등식 2의 손실 함수의 순서를 최적화하며, Q-네트워크의 매개 변수를 업데이트하기 위해 RPROP 알고리즘을 사용합니다. 그러나 이는 데이터 세트의 크기에 비례하여 반복당 계산 비용이 있는 배치 업데이트를 사용하며, 우리는 반복당 저비용의 확률적 경사 업데이트를 고려합니다. NFQ는 또한 단순한 시각적 입력을 사용하여 간단한 실제 제어 작업에 성공적으로 적용되었는데, 이는 먼저 심층 오토인코더를 사용하여 작업의 저차원 표현을 학습한 다음 이 표현에 NFQ를 적용했습니다. 반면, 우리의 접근 방식은 시각적 입력에서 직접 강화 학습을 적용합니다. 결과적으로, 이는 행동 가치를 식별하는 데 직접적으로 관련이 있는 기능을 학습할 수 있습니다. 또한, 경험 재생과 간단한 신경망을 사용하여 Q-learning이 이전에 결합되었지만, 이는 다시 말하지만 저차원 상태로 시작하여 시각적 입력이 아닌 것으로 시작했습니다.

Atari 2600 에뮬레이터를 강화 학습 플랫폼으로 사용한 것은 [3]에 의해 소개되었습니다. 여기서는 선형 함수 근사기와 일반적인 시각적 특징을 사용하여 표준 강화 학습 알고리즘을 적용했습니다. 이후, 특징의 수를 늘리고, 특징을 난수로 하위 차원 공간으로 투영하기 위해 터그오브워 해싱을 사용하여 결과가 개선되었습니다. 하이퍼NEAT 진화 구조 [8]는 또한 Atari 플랫폼에 적용되었는데, 각 게임에 대한 전략을 나타내는 신경망을 진화시키기 위해 사용되었습니다. 에뮬레이터의 리셋 기능을 사용하여 결정적 시퀀스에 대해 반복적으로 훈련시킬 때, 이러한 전략은 여러 Atari 게임의 디자인 결함을 이용할 수 있었습니다.





