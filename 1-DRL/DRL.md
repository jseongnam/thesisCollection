# 제목 : Playing Atari with Deep Reinforcement Learning
## Abstract
강화학습을 이용하여 고차원 감각 입력으로부터 직접 제어정책을 성공적으로 학습한 최초의 딥러닝 모델을 제시한다. 모델은 Q-learning의 변형으로 훈련된 컨볼루션 신경망이다.

입력은 원시 픽셀이고, 출력은 미래를 추정하는 값 함수이다. 보상은 아케이드 학습 환경의 아타리 2600 게임 7개에 아키텍처나 학습 알고리즘을 조정하지 않고 적용한다.

결과 : 6개의 게임에서 이전의 모든 접근 방식을 능가하고 다음을 능가한다는 것을 발견.

## Introduction
이 논문은, 컨볼루션 신경망이 기존의 강화학습의 딥러닝의 문제를 극복하고 학습할 수 있음을 보여준다.

## Background
에이전트가 환경 E와 상호작용하는 작업을 고려한다. 아타리 에뮬레이터, 일련의 행동, 관찰 및 보상, 각 시간 단계에서 에이전트는 행동을 선택한다.
합법적인 게임 액션 집합에서 A = {1,...,K} 액션은 에뮬레이터로 전달되며, 내부 상태와 게임 점수를 수정한다. 일반적으로 E는 확률적일 수 있다.
에뮬레이터의 에이전트가 내부 상태를 관찰하지 않고, 그 대신 이미지 xt ∈ R 을 관찰한다. 게임 점수의 변화를 나타내는 rt가 있다. 일반적응로 게임 점수는 다음과 같다.
행동 및 관찰의 전체 사전 시퀀스 : 행동에 대한 피드백만 수신할 수 있다. 수천 번의 시간이 흐른 뒤에, 에이전트는 현재 화면의 이미지만 관찰하기 때문에, 작업이 부분적으로 관찰되고, 현재 상황을 완전히 이해하는 것은 불가능하다. 현재 화면 xt로부터만, 우리는 행동과 관찰의 순서를 고려한다. 
St =x1, a1, x2, ..., at-1, xt 이 시퀀스들에 의존하는 게임 전략을 배운다. 
에이전트의 목표는 감소율 γ로 할인된 미래 보상을 최대화하는 방식으로 에뮬레이터와 상호작용하는 것.
우리는 일반적으로 미래 보상이 시간 단계마다 γ의 지수적으로 할인된다는 가정을 하며, 시간 t에서 미래 할인된 반환을 다음과 같이 정의한다.

$R_t = \sum_{t_0 = t}^{T} \gamma^{t_0 - t} r_{t_0}$

여기서 T는 게임이 종료되는 시간 단계이다.
우리는 최적의 행동-가치 함수 Q∗(s, a)을 다음과 같이 정의한다.
먼저, 이는 일부 시퀀스 s를 보고 일부 작업 a를 수행한 후에 어떤 전략을 따를 때 얻을 수 있는 최대 기대 반환이다.

$Q^*(s, a) = \max_{\pi} \mathbb{E} [R_t|s_t = s, a_t = a, \pi]$

여기서 $\pi$ 는 시퀀스를 작업(또는 작업에 대한 분포)로 매핑하는 정책이다.
최적의 행동-가치함수는 벨만 방정식이라는 중요한 동일성을 따른다. 이는 다음 직관에 기반한다.
$s_0$ 의 최적 값 $Q_*(s_0,a_0)$ 이 모든 가능한 작업 $a_0$ 에 대해 알려져 있다면, 
최적의 전략은 기대값  $r + γQ_∗(s_0, a_0)$ 을 최대화하는 작업 $a_0$을 선택하는 것.

$Q^*(s, a) = E_{s_0 \sim \epsilon} [r + \gamma \max_{a_0} Q^*(s_0, a_0) | s, a]$

많은 강화 학습 알고리즘의 기본 아이디어는 벨만 방정식을 반복적인 업데이트로 사용하여 행동-가치 함수를 추정하는 것이다.
이러한 값 반복 알고리즘은 i → ∞에 대해 $Q_i$ → $Q_∗$로 수렴한다.
실제로, 이러한 기본 접근 방식은 각 시퀀스에 대해 행동-가치 함수를 별도로 추정하므로 전혀 실용적이지 못하다.
대신, 행동-가치 함수를 추정하기 위해 함수 근사기를 사용하는 것이 일반적이다. 
이 때 행동-가치 함수는 Q(s, a; θ) ≈ Q∗(s, a) 로 표기된다.
강화학습 커뮤니티에서는 일반적으로 선형 함수 근사기를 사용하지만, 때로는 신경망과 같은 비선형 함수 근사기를 대신 사용한다.

우리는 가중치 θ 를 가진 신경망 함수 근사기를 Q-network 이라고 한다. Q-network은 각 반복에서 변하는 손실 함수 $L_i(θ_i)$ 를 최소화하여 훈련될 수 있다.

$L_i (θ_i) = E_{s,a∼ρ(·)}[(y_i − Q (s, a; θ_i))2]$ 
여기서 $y_i = E_{s0∼E} [r + γ max_{a0} Q(s_0, a_0; θ_{i−1})|s, a]$

## Related Work

강화 학습의 가장 잘 알려진 성공 사례 중 하나는 TD-Gammon입니다. 이는 완전히 강화 학습과 셀프 플레이로 학습한 배경모노 게임 프로그램으로, 초인적인 수준의 게임 능력을 달성했습니다. TD-Gammon은 Q-learning과 유사한 모델 없는 강화 학습 알고리즘을 사용하고, 한 개의 은닉층을 가진 다층 퍼셉트론을 사용하여 가치 함수를 근사했습니다.

그러나, TD-Gammon에 대한 조사를 이어가는 초기 시도는 체스, 바둑, 체커와 같은 게임에 동일한 방법을 적용한 것들은 덜 성공적이었습니다. 이로 인해 TD-Gammon 접근법이 배경모노에서만 작동하는 특수한 경우라는 보편적인 믿음이 생겼는데, 아마도 주사위 굴림의 확률성이 상태 공간을 탐색하고 가치 함수를 특히 부드럽게 만든다는 이유 때문일 것입니다.

또한, Q-learning과 같은 모델 없는 강화 학습 알고리즘을 비선형 함수 근사기와 결합하거나, 오프-정책 학습과 같은 것을 함께 사용할 경우 Q-네트워크가 발산할 수 있다는 것이 밝혀졌습니다. 이후, 강화 학습 분야의 주요 작업은 수렴 보장이 더 우수한 선형 함수 근사기에 중점을 두었습니다.

더 최근에는 심층 학습과 강화 학습을 결합하는 데 관심이 다시 생겼습니다. 심층 신경망은 환경 E를 추정하는 데 사용되었으며, 제한된 볼츠만 머신은 가치 함수나 정책을 추정하는 데 사용되었습니다. 또한, Q-learning의 발산 문제는 경사 시간 차 메서드를 통해 일부 해결되었습니다. 이러한 방법은 비선형 함수 근사기를 사용하여 고정된 정책을 평가할 때 수렴하거나, Q-learning의 제한된 변형을 사용하여 선형 함수 근사기를 사용하여 제어 정책을 학습할 때 수렴함이 입증되었습니다. 그러나 이러한 방법은 아직 비선형 제어로 확장되지 않았습니다.

우리의 접근 방식과 가장 유사한 이전 연구는 신경 맞춤 Q-learning (NFQ)입니다. NFQ는 등식 2의 손실 함수의 순서를 최적화하며, Q-네트워크의 매개 변수를 업데이트하기 위해 RPROP 알고리즘을 사용합니다. 그러나 이는 데이터 세트의 크기에 비례하여 반복당 계산 비용이 있는 배치 업데이트를 사용하며, 우리는 반복당 저비용의 확률적 경사 업데이트를 고려합니다. NFQ는 또한 단순한 시각적 입력을 사용하여 간단한 실제 제어 작업에 성공적으로 적용되었는데, 이는 먼저 심층 오토인코더를 사용하여 작업의 저차원 표현을 학습한 다음 이 표현에 NFQ를 적용했습니다. 반면, 우리의 접근 방식은 시각적 입력에서 직접 강화 학습을 적용합니다. 결과적으로, 이는 행동 가치를 식별하는 데 직접적으로 관련이 있는 기능을 학습할 수 있습니다. 또한, 경험 재생과 간단한 신경망을 사용하여 Q-learning이 이전에 결합되었지만, 이는 다시 말하지만 저차원 상태로 시작하여 시각적 입력이 아닌 것으로 시작했습니다.

Atari 2600 에뮬레이터를 강화 학습 플랫폼으로 사용한 것은 [3]에 의해 소개되었습니다. 여기서는 선형 함수 근사기와 일반적인 시각적 특징을 사용하여 표준 강화 학습 알고리즘을 적용했습니다. 이후, 특징의 수를 늘리고, 특징을 난수로 하위 차원 공간으로 투영하기 위해 터그오브워 해싱을 사용하여 결과가 개선되었습니다. 하이퍼NEAT 진화 구조 [8]는 또한 Atari 플랫폼에 적용되었는데, 각 게임에 대한 전략을 나타내는 신경망을 진화시키기 위해 사용되었습니다. 에뮬레이터의 리셋 기능을 사용하여 결정적 시퀀스에 대해 반복적으로 훈련시킬 때, 이러한 전략은 여러 Atari 게임의 디자인 결함을 이용할 수 있었습니다.

## Deep Reinforcement Learning

최근 컴퓨터 비전 및 음성 인식 분야의 획기적인 발전은 매우 큰 교육 데이터 세트에서 심층 신경망을 효율적으로 훈련시키는 데 의존했습니다. 가장 성공적인 접근 방법은 확률적 경사 하강을 기반으로 하는 가벼운 업데이트를 사용하여 원시 입력을 직접 학습합니다. 심층 신경망에 충분한 데이터를 제공하면 종종 수작업 특성보다 나은 표현을 학습할 수 있습니다. 이러한 성공은 우리의 강화 학습 접근 방식을 동기부여합니다. 우리의 목표는 강화 학습 알고리즘을 딥 신경망에 연결하여 RGB 이미지에서 직접 작동하고, 확률적 경사 업데이트를 사용하여 효율적으로 훈련 데이터를 처리하는 것입니다. Tesauro의 TD-Gammon 아키텍처는 그러한 접근 방식에 대한 출발점을 제공합니다. 이 아키텍처는 정책 샘플인 경험 (st, at, rt, st+1, at+1)을 환경과의 상호 작용에서 그 알고리즘의 상호 작용에서 직접 업데이트하는 네트워크의 매개 변수를 업데이트합니다(백거몬의 경우 셀프 플레이). 이 접근 방식은 20년 전에 최고의 인간 배경몬 플레이어를 능가할 수 있었으므로, 20년 동안의 하드웨어 개선과 현대적인 딥 신경망 아키텍처 및 확장 가능한 RL 알고리즘이 상당한 진전을 이룰 수 있는지 궁금할 수 있습니다.
TD-Gammon 및 유사한 온라인 접근 방식과 달리, 우리는 경험 재생이라는 기술을 사용합니다. 여기서 우리는 각 시간 단계의 에이전트의 경험 (et = (st, at, rt, st+1))을 여러 에피소드에서 수집된 데이터 집합 D = e1, ..., eN에 저장합니다. 알고리즘의 내부 루프에서는, 우리는 저장된 샘플 풀에서 무작위로 추출된 경험 e ∼ D에 대해 Q-learning 업데이트 또는 미니배치 업데이트를 적용합니다. 경험 재생을 수행 한 후 에이전트는 -greedy 정책에 따라 작동하고 실행할 동작을 선택합니다. 임의의 길이의 기록을 신경망의 입력으로 사용하는 것은 어려울 수 있으므로, 우리의 Q 함수는 대신 함수 φ에 의해 생성된 기록의 고정된 길이 표현에서 작동합니다. 우리가 딥 Q-learning이라고 부르는 전체 알고리즘은 알고리즘 1에 제시되어 있습니다.
이 접근 방식은 표준 온라인 Q-learning [23]보다 여러 가지 장점이 있습니다. 첫째, 경험의 각 단계는 많은 가중치 업데이트에서 사용될 수 있으며, 이는 더 큰 데이터 효율성을 가능하게 합니다.

## Deep Reinforcement Learning



## Preprocessing and Model Architecture

원시데이터를 직접 처리하기엔 너무 크므로, 입력 차원을 줄이기 위한 전처리 수행. 
1. RGB -> 그레이스케일
2. 110 x 84 이미지로 다운샘플링 
3. 84x84 이미지 영역을 잘라내어 얻는다.

이후 스트라이드 4를 사용하여 이미지에 합성곱을 적용 -> 84 x 84 x 4

이 접근 방식으로 훈련된 합성곱 네트워크 -> Deep Q-Networks 이라고 함.

## Experiments

이후, 7개 게임에 동일한 네트워크 아키텍처, 학습 알고리즘, 하이퍼 파라미터 설정을 사용하여, 다양한 게임에서 작동함을 보여줌.
모든 양의 보상을 1, 음의 보상을 -1로 고정. -> 오차 미분의 크기를 제한하고, 여러 게임에 걸쳐 동일한 학습률을 사용하기 쉽게 만듬.

이 실험에서는, 크기가 32인 미니배치 사용, RMSProp 알고리즘 사용. 동작정책 : $\epsilon$ - greedy 정책.
결과 : 첫 번쨰 백만 프레임에 대해 1에서 0.1로 선형적으로 줄어든 이후, 고정.

에이전트는 k번째 프레임마다 행동을 볼 수 있고, 선택할 수 있으며, 마지막 행동은 건너뛴 프레임에서 반복.
한 단계만 실행하는 것이 에이전트가 행동을 선택하는 것보다 훨씬 적은 계산이 필요하기 때문에, 이 방법은 에이전트가 실행 시간을 크게 늘리지 않고도 대략적으로 k배 더 많은 게임을 할 수 있게 함.
Space Invaders 게임을 제외한 모든 게임에 대해 k = 4를 사용했음.

## Training and Stability

지도학습에서와 달리, 강화학습에서는 훈련 중 에이전트의 진행을 정확하게 평가하는 것이 어려울 수 있음.
여기에서의 평가지표는 에피소드 또는 게임에서 에이전트가 수집한 총 보상을 일정한 수의 게임에 대해 평균화한 것, 따라서, 주기적으로 훈련 중에 이를 계산함.
평균 총 보상 지표는 정책의 가중치에 작은 변화가 생길 때 정책이 방문하는 상태의 분포에 큰 변화를 일으킬 수 있기 때문에, 매우 노이즈가 많음.
하지만, 예측된 Q을 보면, 상대적으로 부드러운 개선을 볼 수 있고, 발산 문제가 없음.
이는, 이론적인 수렴 보장이 없지만, 강화 학습 신호와 확률적 경사 하강법을 사용하여 큰 신경망을 안정적으로 훈련시킬 수 있음을 시사함.

## Main Evaluation

이 접근방식은, 입력에 대한 사전 지식을 포함하지 않음에도, 모든 게임에서 다른 학습 방법보다 높은 성능을 보인다.
진화 정책 탐색 접근 방식과 비교 -> 이 방법은 성공적인 공략을 나타내는 결정론적 상태 시퀀스를 찾는데 의존함. 이 방식의 훈련 방법은 무작위 변형에 일반화될 가능성이 적음.
하지만, 해당 논문의 알고리즘은 탐욕 제어 시퀀스에서 평가되며, 따라서 다양한 가능한 상황에서 일반화됨. 또한, 최대 평가 결과와 평균 결과도 더 나은 성능을 달성.
마지막으로, 이 방법이, Breakout, Enduro, Pong에서 전문가 인간 플레이어보다 더 나은 성능을 달성.

## Conclusion

이 논문은 강화 학습을 위한 새로운 심층 학습 모델을 소개했음.
원시 픽셀만을 입력으로 사용하여, Atari 2600 컴퓨터 게임에 해당하는 제어 정책을 습득하는 능력을 증명함.
또한, 확률적 미니배치 업데이트를 결합한 온라인 Q-학습의 변형을 제시하여, RL에 대한 심층 신경망의 훈련을 용이하게 했음.
하이퍼 파라미터, 아키텍처의 조정 없이, 테스트한 7개 게임 중 6개에서 최고의 결과를 제공했음.